# Complete Project Summary: Medical Triage Prediction System

## üéØ Project Overview

A comprehensive **end-to-end medical triage prediction system** that combines:
- **Synthetic data generation** (550 ‚Üí 20,000 samples)
- **Machine learning ensemble models** (XGBoost, LightGBM, CatBoost)
- **Rule-based triage engine** (clinical safety rules)
- **LLM-powered explanations** (Groq LLaMA-3)
- **Production-ready FastAPI service**

The system predicts **Risk Level** (Low/Medium/High) and **Department** assignment for emergency triage scenarios.

---

## üìÅ Project Structure

### Core Components

1. **Data Generation & Preparation**
   - `generate_synthetic_data.py` - SDV-based synthetic data generation
   - `update_dataset.py` - Data cleaning and formatting
   - `triage_seed_dataset_with_edges.csv` - Original 550 samples
   - `triage_synthetic_dataset_20k.csv` - Generated 20,000 samples

2. **Model Training**
   - `train_triage_models.py` - Complete ML training pipeline
   - `models/` - Saved models and artifacts

3. **Inference & Prediction**
   - `triage_inference.py` - Rule engine + ML ensemble + LLM prompt generation
   - `predict_triage.py` - Standalone prediction script
   - `groq_llm.py` - Groq LLM integration

4. **API Service**
   - `app.py` - FastAPI application with REST endpoints

5. **Documentation**
   - `API_README.md` - API usage guide
   - `ENV_SETUP.md` - Environment variable setup
   - `COLAB_SETUP.md` - Google Colab training instructions

---

## üî¨ Feature 1: Synthetic Data Generation

### File: `generate_synthetic_data.py`

**Purpose:** Generate 20,000 high-quality synthetic samples from 550 seed samples

**Features:**
- Uses **SDV (Synthetic Data Vault)** with **CTGAN** model
- Handles mixed data types (numerical + categorical)
- Preserves statistical properties and relationships
- Generates unique PatientIDs (P00001-P20000)
- Validates synthetic data quality:
  - Column presence check
  - Data type validation
  - Numerical range verification
  - Categorical distribution analysis

**Output:**
- `triage_synthetic_dataset_20k.csv` - 20,000 synthetic patient records

**Data Schema:**
- PatientID, Age, Gender, Chief_Complaint, Symptoms
- Vital Signs: Systolic_BP, Heart_Rate, Body_Temperature_C, Respiratory_Rate, SpO2
- Medical History: Allergies, Current_Medications, Pre_Existing_Conditions
- Triage Info: Risk_Level, Department

---

## üßπ Feature 2: Data Cleaning & Formatting

### File: `update_dataset.py`

**Purpose:** Clean and standardize the synthetic dataset

**Features:**
- **PatientID Format:** Converts to P00001, P00002, ... P20000 format
- **Data Type Conversion:**
  - Age ‚Üí integer (no decimals)
  - SpO2 ‚Üí integer (no decimals)
- **Gender Normalization:** Replaces "Transgender" with "T"
- **Missing Value Handling:** Fills NaN with median for numerical columns

**Output:** Updated `triage_synthetic_dataset_20k.csv` with standardized format

---

## ü§ñ Feature 3: Machine Learning Training Pipeline

### File: `train_triage_models.py`

**Purpose:** Train ensemble ML models for risk prediction and department assignment

### 3.1 Data Loading & Cleaning
- Loads CSV with 20,000 samples
- Handles empty strings and whitespace as NaN
- Replaces missing values in string columns (Allergies, Medications, Conditions) with "None"
- Converts numeric columns with error coercion
- Normalizes Onset_Duration_Unit (hour‚Üíhours, day‚Üídays, etc.)
- Drops rows with NaN in critical numeric features

### 3.2 Feature Engineering
**Features Used:**
- Numerical: Age, Systolic_BP, Heart_Rate, Body_Temperature_C, Respiratory_Rate, SpO2, Onset_Duration_Value
- Categorical: Gender, Chief_Complaint, Onset_Duration_Unit

**Targets:**
- Risk_Level: Low/Medium/High (multiclass)
- Department: General_Medicine, Emergency, Cardiology, Neurology, Orthopedics, Pulmonology

### 3.3 Preprocessing Pipeline
- **ColumnTransformer** with:
  - Numerical features: Pass-through (no transformation)
  - Categorical features: **OneHotEncoder** with `handle_unknown='ignore'`
- Fitted once and reused for all models

### 3.4 Class Imbalance Handling
- **Computes class weights** using `sklearn.utils.class_weight.compute_class_weight('balanced')`
- **Risk_Level weights:** Automatically balanced for Low/Medium/High
- **Department weights:** Automatically balanced for all 6 departments
- **Sample weights applied to:**
  - XGBoost: via `sample_weight` parameter
  - LightGBM: via `sample_weight` parameter
  - CatBoost: via `class_weights` parameter (list format)

### 3.5 Model Training

#### Risk_Level Models (Ensemble):
1. **XGBoost:**
   - 200 estimators, max_depth=5, learning_rate=0.1
   - Trained with sample weights for class imbalance

2. **LightGBM:**
   - 200 estimators, max_depth=5, learning_rate=0.1
   - Trained with sample weights

3. **CatBoost:**
   - 200 iterations, depth=5, learning_rate=0.1
   - Bootstrap type: Bernoulli (supports subsample)
   - Trained with class_weights parameter

#### Department Model:
- **XGBoost:**
  - 200 estimators, max_depth=6, learning_rate=0.1
  - Trained with sample weights

### 3.6 Ensemble Prediction
- Averages probabilities from all 3 risk models: `(p_xgb + p_lgbm + p_cat) / 3`
- Takes argmax for final risk level prediction
- Improves robustness and accuracy

### 3.7 Evaluation Metrics
- **Classification reports** for each individual model
- **Ensemble classification report**
- **Macro F1 score**
- **Per-class recall** (especially important for High risk and minority departments)
- **Per-class precision and F1**

### 3.8 Model Persistence
**Saved Artifacts:**
- `models/preprocessor.joblib` - Data preprocessing pipeline
- `models/risk_xgb.joblib` - XGBoost risk model (pipeline)
- `models/risk_lgbm.joblib` - LightGBM risk model (pipeline)
- `models/risk_cat.joblib` - CatBoost risk model (pipeline)
- `models/dept_xgb.joblib` - XGBoost department model (pipeline)
- `models/risk_label_encoder.joblib` - Risk level label encoder
- `models/dept_label_encoder.joblib` - Department label encoder

**All models wrapped in Pipelines** for seamless preprocessing during inference

---

## üß† Feature 4: Rule-Based Triage Engine

### File: `triage_inference.py` ‚Üí `apply_rules()`

**Purpose:** Apply clinical safety rules before ML prediction

### Rule 1: Invalid Vitals Check
- **Trigger:** Heart_Rate ‚â§ 0 OR Respiratory_Rate ‚â§ 0 OR Systolic_BP ‚â§ 0
- **Action:**
  - `forced = True`
  - `risk_level = "Medium"`
  - `department = "General_Medicine"`
  - `rule_reasons = ["Invalid vital signs (non-positive values). Ask to recheck inputs."]`

### Rule 2: High-Risk Emergency Override
- **Triggers:**
  - Systolic_BP < 80 ‚Üí "Systolic BP < 80 (shock risk)"
  - SpO2 < 88 ‚Üí "SpO2 < 88 (severe hypoxia)"
  - Heart_Rate > 140 OR < 40 ‚Üí "Extreme heart rate"
  - Respiratory_Rate > 35 OR < 8 ‚Üí "Extreme respiratory rate"
  - Chief complaint contains: "severe chest pain", "crushing chest pain", "acute chest pain", "acute shortness of breath", "difficulty breathing", "stroke", "sudden weakness", "loss of consciousness"
- **Action:**
  - `forced = True`
  - `risk_level = "High"`
  - `department = "Emergency"`
  - `rule_reasons = [list of triggered reasons]`

### Rule 3: Chronic But Stable
- **Conditions:**
  - Onset_Duration_Unit in ["months", "years"]
  - 90 ‚â§ Systolic_BP ‚â§ 150
  - 60 ‚â§ Heart_Rate ‚â§ 110
  - 36 ‚â§ Body_Temperature_C ‚â§ 38
  - 92 ‚â§ SpO2 ‚â§ 100
  - No severe emergency keywords in chief complaint
- **Action:**
  - `forced = True`
  - `risk_level = "Low"`
  - `department = "General_Medicine"`
  - `rule_reasons = ["Chronic symptoms with stable vitals"]`

**Benefits:**
- Ensures patient safety (catches critical cases)
- Reduces false negatives for high-risk scenarios
- Handles edge cases ML might miss
- Provides explainable decisions

---

## üîÆ Feature 5: ML Ensemble Prediction

### File: `triage_inference.py` ‚Üí `predict_with_ensemble()`

**Purpose:** Predict using trained ML models when rules don't apply

**Process:**
1. Converts patient dict to pandas DataFrame
2. Applies preprocessing (via pipelines)
3. Gets probabilities from all 3 risk models
4. Averages probabilities: `(p_xgb + p_lgbm + p_cat) / 3`
5. Decodes predictions using label encoders
6. Predicts department using XGBoost model
7. Returns structured prediction with probabilities

**Output:**
```python
{
    "risk_level": "Low",
    "risk_probabilities": {"Low": 0.85, "Medium": 0.12, "High": 0.03},
    "department": "General_Medicine",
    "top_features": []  # Placeholder for future SHAP integration
}
```

---

## üîç Feature 6: SHAP Model Explainability

### File: `triage_inference.py` ‚Üí `get_top_shap_features()`

**Purpose:** Compute SHAP values to identify top contributing features for each prediction

**Features:**
- **SHAP TreeExplainer** for XGBoost Risk_Level model
- **Initialized once at startup** (not per request) for performance
- **Feature name mapping:**
  - Maps preprocessed features back to original feature names
  - Handles OneHotEncoder categorical features (e.g., "Chief_Complaint_chest pain" ‚Üí "Chief Complaint: chest pain")
  - Human-readable feature names for LLM and API responses
- **Top K features:** Returns top 5 features by absolute SHAP value
- **Multi-class handling:** Uses mean absolute SHAP values across all risk classes
- **Error handling:** Gracefully fails to empty list if SHAP computation fails

**Process:**
1. Transform patient data using preprocessor
2. Compute SHAP values for transformed input
3. Extract feature names from preprocessor transformers
4. Sort features by absolute SHAP value (descending)
5. Return top K features with human-readable names

**Output Format:**
```python
[
    ("Systolic BP", 0.45),
    ("SpO2", -0.32),
    ("Chief Complaint: chest pain", 0.28),
    ...
]
```

**Integration:**
- Computed in `predict_with_ensemble()` when ML models are used
- Included in `top_features` field of prediction results
- Fed into LLM prompt for SHAP-aware explanations
- Returned in API response for UI display

**Performance:**
- SHAP explainer initialized once at startup (~1-2 seconds)
- Per-request SHAP computation: ~10-50ms (single row)
- No plots generated (numeric values only)

---

## üí¨ Feature 7: LLM Explanation Generation

### File: `triage_inference.py` ‚Üí `build_llm_explanation_prompt()`

**Purpose:** Generate prompt for LLM to explain triage decisions

**Prompt Includes:**
- Patient summary (age, gender, chief complaint, symptoms, vitals)
- Final risk level and department
- Rule-based reasons (if applicable)
- ML risk probabilities (if applicable)
- Top contributing features (placeholder for SHAP)

**Prompt Structure:**
- System message: "You are a concise clinical triage assistant..."
- User message: Formatted prompt with all relevant information
- Request: 2-3 sentences in simple, non-technical language

### File: `groq_llm.py`

**Purpose:** Interface with Groq's LLaMA-3 API

**Features:**
- **Singleton client pattern** - Initializes Groq client once
- **Environment variable:** Reads `GROQ_API_KEY` from environment
- **Model:** Uses `llama-3.1-8b-instant` (fast inference)
- **Parameters:**
  - Temperature: 0.3 (consistent, factual responses)
  - Max tokens: 200 (concise explanations)
- **Error handling:** Raises exceptions with context

**Function:**
```python
call_llm(prompt: str) -> str
```

---

## üîÑ Feature 7: Complete Inference Pipeline

### File: `triage_inference.py` ‚Üí `predict_risk_and_department()`

**Purpose:** Main entry point combining rules + ML + LLM prompt

**Workflow:**
1. **Apply Rules:** Call `apply_rules(patient)`
2. **If Forced by Rules:**
   - Return rule-based decision
   - Build LLM prompt with rule reasons
   - Set `risk_source = "rules"`, `department_source = "rules"`
3. **If Not Forced:**
   - Run ML ensemble prediction
   - Build LLM prompt with ML probabilities
   - Set `risk_source = "ensemble"`, `department_source = "model"`
4. **Return:** Complete dict with all information + `llm_prompt`

**Output Structure:**
```python
{
    "risk_level": str,
    "risk_source": "rules" | "ensemble",
    "risk_probabilities": Dict[str, float] | None,
    "department": str,
    "department_source": "rules" | "model",
    "rule_reasons": List[str],
    "top_features": List[Tuple[str, float]],
    "llm_prompt": str
}
```

---

## üåê Feature 8: FastAPI REST API

### File: `app.py`

**Purpose:** Production-ready API service for triage predictions

### 8.1 Application Setup
- **FastAPI app** with CORS middleware
- **Automatic .env loading** (via python-dotenv)
- **Structured logging** with Python logging module
- **Pydantic models** for request/response validation

### 8.2 Startup/Shutdown Events
- **Startup:** Loads all ML models and artifacts once (not per request)
- **Shutdown:** Cleanup logging

### 8.3 API Endpoints

#### `GET /`
- Health check endpoint
- Returns service status

#### `GET /health`
- Detailed health check
- Returns models loaded status

#### `POST /triage/predict`
**Request Body:**
```json
{
  "PatientID": "P00001",
  "Age": 45,
  "Gender": "M",
  "Chief_Complaint": "chest pain",
  "Symptoms": "mild discomfort",
  "Systolic_BP": 120,
  "Heart_Rate": 80,
  "Body_Temperature_C": 37.2,
  "Respiratory_Rate": 18,
  "SpO2": 98,
  "Onset_Duration_Value": 2.5,
  "Onset_Duration_Unit": "hours",
  "Allergies": "None",
  "Current_Medications": "Aspirin",
  "Pre_Existing_Conditions": "Hypertension"
}
```

**Response:**
```json
{
  "risk_level": "Low",
  "risk_source": "ensemble",
  "risk_probabilities": {
    "Low": 0.85,
    "Medium": 0.12,
    "High": 0.03
  },
  "department": "General_Medicine",
  "department_source": "model",
  "rule_reasons": [],
  "top_features": [],
  "nl_explanation": "Based on the patient's stable vital signs..."
}
```

**Features:**
- Validates input with Pydantic models
- Calls `predict_risk_and_department()` for prediction
- Calls `call_llm()` for natural language explanation
- **Error handling:**
  - LLM failures ‚Üí Fallback explanation
  - Missing models ‚Üí 503 Service Unavailable
  - Invalid input ‚Üí 400 Bad Request
- Converts tuples to lists for JSON serialization
- Excludes raw `llm_prompt` from response (only returns `nl_explanation`)

### 8.4 API Documentation
- **Swagger UI:** Available at `/docs` when server is running
- **ReDoc:** Available at `/redoc`
- Auto-generated from Pydantic models

---

## üìä Feature 9: Model Loading & Artifact Management

### File: `triage_inference.py` ‚Üí `load_artifacts()`

**Purpose:** Load all saved models and encoders

**Features:**
- Loads preprocessor, all 3 risk models, department model, label encoders
- Extracts classifiers from pipelines while preserving pipeline structure
- Supports custom model directory
- Error handling for missing files

**Returns:**
```python
{
    "preprocessor": ColumnTransformer,
    "risk_xgb": XGBClassifier,
    "risk_lgbm": LGBMClassifier,
    "risk_cat": CatBoostClassifier,
    "dept_xgb": XGBClassifier,
    "risk_xgb_pipe": Pipeline,  # Full pipeline for prediction
    "risk_lgbm_pipe": Pipeline,
    "risk_cat_pipe": Pipeline,
    "dept_xgb_pipe": Pipeline,
    "risk_label_encoder": LabelEncoder,
    "dept_label_encoder": LabelEncoder
}
```

---

## üõ†Ô∏è Feature 10: Standalone Prediction Script

### File: `predict_triage.py`

**Purpose:** Command-line prediction tool (without API)

**Features:**
- Lazy loading of models (loads on first use)
- Simple function interface: `predict_risk_and_department(patient_dict)`
- Example usage included
- Useful for testing and debugging

---

## üì¶ Dependencies

### Core ML Libraries
- `pandas>=2.0.0` - Data manipulation
- `numpy>=1.24.0` - Numerical operations
- `scikit-learn>=1.3.0` - ML algorithms and preprocessing
- `xgboost>=2.0.0` - Gradient boosting
- `lightgbm>=4.0.0` - Light gradient boosting
- `catboost>=1.2.0` - Categorical boosting
- `joblib>=1.3.0` - Model serialization

### API & Web
- `fastapi>=0.104.0` - Web framework
- `uvicorn[standard]>=0.24.0` - ASGI server
- `pydantic>=2.0.0` - Data validation

### LLM Integration
- `groq>=0.4.0` - Groq API client

### Explainability
- `shap>=0.43.0` - SHAP values for model interpretability

### Utilities
- `python-dotenv>=1.0.0` - Environment variable management

---

## üéØ Key Features Summary

### Data Pipeline
‚úÖ Synthetic data generation (550 ‚Üí 20,000 samples)  
‚úÖ Data cleaning and standardization  
‚úÖ Missing value handling  
‚úÖ Type conversion and normalization  

### Machine Learning
‚úÖ Ensemble models (XGBoost + LightGBM + CatBoost)  
‚úÖ Class imbalance handling with sample weights  
‚úÖ Preprocessing pipeline with ColumnTransformer  
‚úÖ Label encoding for categorical targets  
‚úÖ Model persistence with joblib  
‚úÖ Comprehensive evaluation metrics  

### Rule Engine
‚úÖ Invalid vitals detection  
‚úÖ High-risk emergency override  
‚úÖ Chronic stable case handling  
‚úÖ Explainable rule-based decisions  

### Model Explainability
‚úÖ SHAP-based feature contributions  
‚úÖ Top K features per prediction  
‚úÖ Human-readable feature names  
‚úÖ SHAP values integrated into LLM prompts  

### LLM Integration
‚úÖ Groq LLaMA-3 integration  
‚úÖ Natural language explanations  
‚úÖ SHAP-aware explanations (mentions top contributing factors)  
‚úÖ Fallback handling for LLM failures  
‚úÖ Configurable via environment variables  

### API Service
‚úÖ FastAPI REST API  
‚úÖ Request/response validation  
‚úÖ Error handling  
‚úÖ Health check endpoints  
‚úÖ Auto-generated documentation  
‚úÖ CORS support  
‚úÖ Models loaded once at startup  

### Production Features
‚úÖ Environment variable management  
‚úÖ Structured logging  
‚úÖ Type hints throughout  
‚úÖ Comprehensive error handling  
‚úÖ Security best practices (.gitignore for .env)  

---

## üìà Performance Characteristics

### Training
- **Dataset:** 20,000 samples
- **Train/Test Split:** 80/20 (16,000 / 4,000)
- **Training Time:** ~5-10 minutes (depending on hardware)
- **Models:** 3 risk models + 1 department model

### Inference
- **Rule Engine:** <1ms (pure Python logic)
- **ML Prediction:** ~10-50ms (ensemble + preprocessing)
- **SHAP Computation:** ~10-50ms (single row, per request)
- **LLM Explanation:** ~200-500ms (Groq API call)
- **Total API Response:** ~270-650ms (including SHAP)

### Model Accuracy
- **Risk Level:** ~83% accuracy with balanced recall for all classes
- **Department:** Balanced predictions across all 6 departments
- **High Risk Recall:** Improved from 0.0 to non-zero with class weights
- **Minority Department Recall:** Improved from 0.0 to non-zero

---

## üîí Security & Best Practices

- ‚úÖ `.env` file in `.gitignore` (API keys not committed)
- ‚úÖ Environment variable validation
- ‚úÖ Input validation with Pydantic
- ‚úÖ Error handling with safe fallbacks
- ‚úÖ Structured logging (no sensitive data)
- ‚úÖ CORS configuration (configurable origins)

---

## üìö Documentation Files

1. **API_README.md** - API usage guide with examples
2. **ENV_SETUP.md** - Environment variable setup instructions
3. **COLAB_SETUP.md** - Google Colab training instructions
4. **PROJECT_SUMMARY.md** - This comprehensive summary

---

## üöÄ Quick Start

### 1. Train Models
```bash
python train_triage_models.py
```

### 2. Set Environment Variable
```bash
export GROQ_API_KEY="your-api-key"
# Or create .env file
```

### 3. Run API
```bash
python app.py
```

### 4. Test API
```bash
curl -X POST "http://localhost:8000/triage/predict" \
  -H "Content-Type: application/json" \
  -d '{"Age": 45, "Gender": "M", ...}'
```

---

## üéì System Architecture

```
Patient Data
    ‚Üì
[Rule Engine] ‚Üí Forced? ‚Üí Yes ‚Üí Rule-based Decision
    ‚Üì No
[ML Ensemble] ‚Üí XGBoost + LightGBM + CatBoost
    ‚Üì
[SHAP Explainability] ‚Üí Top 5 Contributing Features
    ‚Üì
[Department Model] ‚Üí XGBoost
    ‚Üì
[LLM Explanation] ‚Üí Groq LLaMA-3 (SHAP-aware)
    ‚Üì
API Response (JSON)
```

---

## üéØ Feature 11: Active Questioning Triage Flow

### File: `active_questioning.py`

**Purpose:** Progressive questioning system that asks the most informative questions based on feature importance

**Features:**
- **Deterministic, greedy strategy** (NOT reinforcement learning):
  - Questions selected based on global SHAP feature importance
  - Higher importance features asked first
  - No RL libraries or Q-learning
- **Question configuration:** Human-readable questions for each feature
- **Session management:** Tracks known answers, asked questions, and session state
- **Stopping conditions:**
  - Confidence threshold reached (max risk probability ‚â• threshold)
  - Maximum questions asked
  - Rule-based decision triggered (immediate stop)
- **Integration:** Reuses existing rule engine and ML ensemble

**Question Selection:**
1. Computes global feature importance using SHAP on sample data
2. Ranks features by mean absolute SHAP value
3. Selects highest-importance unasked feature
4. Stops when confidence threshold reached or max questions asked

**Session Flow:**
1. **Start:** Initialize with minimal info (Age, Gender, Chief_Complaint)
2. **Question:** System asks next most important question
3. **Answer:** User provides answer
4. **Predict:** System updates prediction with new information
5. **Repeat:** Until done (confidence reached or max questions)
6. **Final:** Returns final prediction with LLM explanation

**API Endpoints:**
- `POST /triage/session/start` - Start new session
- `POST /triage/session/next` - Answer question and get next
- `DELETE /triage/session/{session_id}` - End session

**Benefits:**
- Reduces information gathering time
- Focuses on most critical information first
- Provides real-time risk assessment during questioning
- Stops early when confident enough

---

## ‚ú® Unique Selling Points

1. **Hybrid Approach:** Rules + ML ensures safety and accuracy
2. **Explainable AI:** SHAP + LLM provides both technical and natural language explanations
3. **SHAP-Aware Explanations:** LLM uses top contributing features for grounded explanations
4. **Active Questioning:** Progressive questioning based on feature importance
5. **Production Ready:** FastAPI with proper error handling
6. **Class Imbalance Handled:** Balanced recall for all classes
7. **Scalable:** Models loaded once, fast inference
8. **Comprehensive:** End-to-end from data generation to API

---

This is a **complete, production-ready medical triage prediction system** with synthetic data generation, ensemble ML models, rule-based safety checks, SHAP-based explainability, and LLM-powered explanations, all accessible via a RESTful API.

---

## üîç Feature 6 Details: SHAP Explainability

### SHAP Integration Architecture

**Initialization:**
- SHAP TreeExplainer created once at startup in `load_artifacts()`
- Uses XGBoost Risk_Level model (extracted from pipeline)
- Stored in artifacts dict for reuse across requests

**Feature Extraction:**
- `get_top_shap_features()` function computes SHAP values per patient
- Handles multi-class output (averages across risk classes)
- Maps preprocessed feature indices back to original feature names
- Extracts categorical feature names from OneHotEncoder

**Feature Name Mapping:**
- **Numeric features:** Direct mapping (Age, Systolic_BP, etc.)
- **Categorical features:** 
  - OneHotEncoder format: `Chief_Complaint_chest pain`
  - Human-readable: `Chief Complaint: chest pain`
  - Pattern: `{Original_Column}_{Encoded_Value}` ‚Üí `{Original Column}: {Encoded Value}`

**LLM Integration:**
- SHAP values classified by magnitude:
  - `> 0.3`: "strongly increases/decreases"
  - `> 0.15`: "moderately increases/decreases"
  - `> 0.05`: "slightly increases/decreases"
  - `‚â§ 0.05`: "minimally affects"
- Patient values included when available (e.g., "Systolic BP = 78 (strongly increases risk)")
- LLM prompt includes top features for context-aware explanations

**API Response:**
- `top_features` field contains list of `[feature_name, shap_value]` pairs
- Backward compatible (empty list if SHAP unavailable or rule-based)
- UI can display feature contributions alongside LLM explanation

**Error Handling:**
- SHAP initialization failure: explainer set to None, system continues
- SHAP computation failure: returns empty list, prediction continues
- No impact on core prediction functionality
